# World University Scraper

Sistema completo de scraping, procesamiento y exportaciÃ³n de datos de rankings universitarios de Times Higher Education (THE).

## ğŸ—ï¸ Arquitectura del Sistema

```
world_university_scraper/
â”œâ”€â”€ README.md
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ setup.py                    # ğŸ”¥ SCRIPT DE CONFIGURACIÃ“N INICIAL
â”œâ”€â”€ config/                     # Configuraciones
â”‚   â”œâ”€â”€ default_selenium.yml    # Config principal de rankings
â”‚   â”œâ”€â”€ university_detail.yml   # Config de detalles universitarios
â”‚   â””â”€â”€ full_pipeline.yml       # Config del pipeline completo
â”œâ”€â”€ scripts/                    # Scripts especializados
â”‚   â”œâ”€â”€ scrape_rankings.py      # Scraper principal de rankings
â”‚   â””â”€â”€ scrape_universities.py  # Scraper de detalles universitarios
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # HTML y JSON sin procesar
â”‚   â”œâ”€â”€ processed/              # Datos procesados (pickle, CSV)
â”‚   â”œâ”€â”€ universities/           # Datos detallados de universidades
â”‚   â”œâ”€â”€ exports/                # Datos exportados (CSV, Excel, JSON)
â”‚   â”‚   â”œâ”€â”€ csv/               # Exportaciones CSV
â”‚   â”‚   â”œâ”€â”€ excel/             # Exportaciones Excel
â”‚   â”‚   â””â”€â”€ json/              # Exportaciones JSON
â”‚   â”œâ”€â”€ pipeline_results/       # Resultados de pipeline completo
â”‚   â””â”€â”€ backups/               # Backups automÃ¡ticos
â”œâ”€â”€ logs/                       # Archivos de log
â”‚   â”œâ”€â”€ main_orchestrator.log   # Log del orquestador principal
â”‚   â”œâ”€â”€ rankings_scraper.log    # Log del scraping de rankings
â”‚   â””â”€â”€ university_scraper.log  # Log del scraping de universidades
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __main__.py            # ğŸ”¥ ORQUESTADOR PRINCIPAL
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ pipeline.py         # Pipeline de rankings
â”‚   â”‚   â””â”€â”€ university_pipeline.py  # Pipeline de universidades
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ rankings_scraper.py
â”‚   â”‚   â”œâ”€â”€ selenium_rankings_scraper.py
â”‚   â”‚   â””â”€â”€ university_detail_scraper.py  # ğŸ”¥ NUEVO
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ rankings_parser.py   # ğŸ”¥ MEJORADO
â”‚   â”‚   â””â”€â”€ university_detail_parser.py   # ğŸ”¥ NUEVO
â”‚   â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ exporters/
â”‚   â”œâ”€â”€ storage/
â”‚   â””â”€â”€ utils/
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos

- Python 3.11+
- [Poetry](https://python-poetry.org/) para gestiÃ³n de dependencias
- Google Chrome (para Selenium)

### ğŸ”§ ConfiguraciÃ³n Inicial AutomÃ¡tica (Recomendado)

```bash
# 1. Clonar el repositorio
git clone <repository-url>
cd world_university_scraper

# 2. Instalar dependencias
poetry install
poetry shell

# 3. ğŸ”¥ EJECUTAR CONFIGURACIÃ“N AUTOMÃTICA
python setup.py
```

El script `setup.py` automÃ¡ticamente:

- âœ… Crea toda la estructura de directorios necesaria
- âœ… Genera archivos de configuraciÃ³n template
- âœ… Crea `.env.template` con variables de entorno
- âœ… Configura `.gitignore` apropiado
- âœ… Muestra los siguientes pasos de implementaciÃ³n

### ğŸ“‹ DespuÃ©s de Ejecutar setup.py

#### 1. Copiar Archivos de CÃ³digo

```bash
# REEMPLAZAR archivos existentes (versiones mejoradas)
cp <provided>/rankings_parser.py src/parsers/rankings_parser.py
cp <provided>/__main__.py src/__main__.py

# CREAR archivos nuevos
cp <provided>/university_detail_scraper.py src/scrapers/
cp <provided>/university_detail_parser.py src/parsers/
cp <provided>/university_pipeline.py src/core/
cp <provided>/scrape_rankings.py scripts/
cp <provided>/scrape_universities.py scripts/
cp <provided>/university_detail.yml config/
cp <provided>/full_pipeline.yml config/
```

#### 2. Configurar Variables de Entorno

```bash
# Copiar template y personalizar
cp .env.template .env
# Editar .env con tus valores reales
```

### ğŸ”„ ConfiguraciÃ³n Manual (Alternativa)

Si prefieres configurar manualmente:

```bash
# Crear directorios
mkdir -p logs data/{raw,processed,universities,exports/{csv,excel,json},pipeline_results,backups} scripts

# Instalar dependencias
poetry install
poetry shell
```

### ConfiguraciÃ³n de Base de Datos (Opcional)

```bash
# Para exportar a PostgreSQL
export POSTGRES_PASSWORD="tu_password_seguro"

# O en archivo .env
echo "POSTGRES_PASSWORD=tu_password_seguro" >> .env
```

## âœ… VerificaciÃ³n de InstalaciÃ³n

### Prueba RÃ¡pida

```bash
# Verificar que todo estÃ¡ correctamente instalado
python -m src --mode rankings-only --limit 5 --log-level DEBUG --dry-run

# Primera ejecuciÃ³n real (datos limitados)
python -m src --mode rankings-only --limit 5 --log-level DEBUG
```

### DiagnÃ³stico de Problemas

```bash
# Verificar configuraciÃ³n
python setup.py  # Ejecutar nuevamente para verificar

# Ver estructura de directorios creada
tree data/ logs/ scripts/ config/

# Verificar importaciones
python -c "from src.scrapers import UniversityDetailScraper; print('âœ“ Imports OK')"
python -c "from src.parsers import UniversityDetailParser; print('âœ“ Imports OK')"
```

## ğŸ¯ Modos de EjecuciÃ³n

El sistema ofrece mÃºltiples modos de ejecuciÃ³n a travÃ©s del orquestador principal:

### 1. ğŸ”¥ Orquestador Principal con Auto-InserciÃ³n PostgreSQL

```bash
# Pipeline completo con auto-inserciÃ³n a PostgreSQL
python -m src --mode full-pipeline --config config/full_pipeline.yml

# Solo rankings con auto-inserciÃ³n a PostgreSQL
python -m src --mode rankings-only --limit 50 --config config/full_pipeline.yml

# Solo universidades con auto-inserciÃ³n a PostgreSQL
python -m src --mode universities-only \
    --rankings-file data/raw/rankings_2025_reputation.json \
    --config config/full_pipeline.yml

# Deshabilitar PostgreSQL temporalmente
python -m src --mode rankings-only --no-postgres --limit 10

# Pipeline completo con exportaciÃ³n tradicional + PostgreSQL
python -m src --mode full-pipeline --export-data --config config/full_pipeline.yml
```

### 2. ğŸ“Š Scripts Especializados

#### Scraping de Rankings

```bash
# BÃ¡sico
python scripts/scrape_rankings.py

# Con parÃ¡metros personalizados
python scripts/scrape_rankings.py \
    --config config/default_selenium.yml \
    --year 2025 \
    --view reputation \
    --process-data \
    --save-pickle \
    --log-level INFO

# Solo guardar HTML para debugging
python scripts/scrape_rankings.py \
    --save-html \
    --no-json \
    --log-level DEBUG
```

#### Scraping de Universidades

```bash
# Desde archivo de rankings
python scripts/scrape_universities.py \
    --rankings-file data/raw/rankings_2025_reputation_20250613_123456.json \
    --config config/university_detail.yml

# Con procesamiento por lotes
python scripts/scrape_universities.py \
    --rankings-file rankings.json \
    --batch-size 25 \
    --log-level INFO

# Para testing (solo primeras 10 universidades)
python scripts/scrape_universities.py \
    --rankings-file rankings.json \
    --limit 10 \
    --log-level DEBUG
```

## ğŸ“‹ Flujos de Trabajo TÃ­picos

### Flujo Completo Automatizado

```bash
# Ejecutar pipeline completo con todas las funcionalidades
python -m src \
    --mode full-pipeline \
    --config config/full_pipeline.yml \
    --export-data \
    --year 2025 \
    --batch-size 50
```

### Flujo por Etapas

```bash
# Paso 1: Scraping de rankings
python -m src --mode rankings-only --config config/default_selenium.yml

# Paso 2: Scraping de universidades (usar el archivo generado en paso 1)
python -m src --mode universities-only \
    --rankings-file data/raw/rankings_2025_reputation_20250613_145230.json

# Paso 3: Exportar a diferentes formatos
python -m src --mode export-only \
    --processed-file data/processed/combined_data_20250613_150000.pkl
```

### Flujo de Desarrollo/Testing

```bash
# Testing rÃ¡pido con datos limitados
python -m src \
    --mode full-pipeline \
    --limit 20 \
    --batch-size 5 \
    --log-level DEBUG \
    --rankings-only-flag  # Solo rankings, sin universidades
```

## âš™ï¸ ConfiguraciÃ³n Detallada

### ConfiguraciÃ³n Principal (`full_pipeline.yml`)

```yaml
general:
  output_dir: 'data/pipeline_results'
  log_level: 'INFO'

# Archivos de configuraciÃ³n para cada componente
rankings_config: 'config/default_selenium.yml'
university_config: 'config/university_detail.yml'

# ConfiguraciÃ³n de exportadores
exporters:
  postgres:
    enabled: true
    host: 'localhost'
    database: 'university_rankings'
    # ... mÃ¡s opciones

  csv:
    enabled: true
    output_dir: 'data/exports/csv'

  excel:
    enabled: false
    output_dir: 'data/exports/excel'
```

### ConfiguraciÃ³n de Rankings (`default_selenium.yml`)

```yaml
scraper:
  type: 'selenium'
  base_url: 'https://www.timeshighereducation.com/world-university-rankings/latest/world-ranking'
  rankings:
    year: '2025'
    view: 'reputation'
  request_delay: 3

selenium:
  headless: true
  page_load_timeout: 60
  save_html: true
```

### ConfiguraciÃ³n de Universidades (`university_detail.yml`)

```yaml
scraper:
  type: 'selenium'
  headless: true
  university_delay: 5 # Delay entre universidades
  max_retries: 3
```

## ğŸ“Š Exportadores Disponibles

El sistema incluye mÃºltiples exportadores para diferentes necesidades:

### âœ… **Exportadores Siempre Disponibles**

- **CSV**: ExportaciÃ³n a archivos CSV estÃ¡ndar
- **JSON**: ExportaciÃ³n a archivos JSON con metadatos opcionales
- **Excel**: ExportaciÃ³n a archivos Excel con mÃºltiples hojas

### ğŸ”§ **Exportadores Opcionales**

- **PostgreSQL**: ExportaciÃ³n a base de datos PostgreSQL (requiere instalaciÃ³n y configuraciÃ³n)

### **Ejemplos de ConfiguraciÃ³n de Exportadores**

#### CSV Export

```yaml
exporters:
  csv:
    enabled: true
    output_dir: 'data/exports/csv'
    filename: 'rankings_{timestamp}.csv'
    encoding: 'utf-8'
    index: false
```

#### JSON Export

```yaml
exporters:
  json:
    enabled: true
    output_dir: 'data/exports/json'
    filename: 'data_{timestamp}.json'
    pretty_print: true
    include_metadata: true # Incluye informaciÃ³n sobre exportaciÃ³n
```

#### Excel Export

```yaml
exporters:
  excel:
    enabled: true
    output_dir: 'data/exports/excel'
    filename: 'report_{timestamp}.xlsx'
    sheet_name: 'University_Data'
    include_summary: true # Hoja adicional con resumen
```

#### PostgreSQL Export (Opcional)

```yaml
exporters:
  postgres:
    enabled: true
    host: 'localhost'
    port: 5432
    database: 'university_rankings'
    user: 'scraper_user'
    password: '${POSTGRES_PASSWORD}' # Variable de entorno
    table_name: 'rankings'
    if_exists: 'replace' # replace, append, fail
```

```yaml
scraper:
  type: 'selenium'
  base_url: 'https://www.timeshighereducation.com/world-university-rankings/latest/world-ranking'
  rankings:
    year: '2025'
    view: 'reputation'
  request_delay: 3

selenium:
  headless: true
  page_load_timeout: 60
  save_html: true
```

## ğŸ“Š Estructura de Datos

### Datos de Rankings (Mejorados)

```json
{
  "rank": 1,
  "name": "University of Oxford",
  "country": "United Kingdom",
  "university_url": "https://www.timeshighereducation.com/world-university-rankings/university-oxford",
  "overall_score": 98.5,
  "teaching_score": 96.8,
  "research_score": 99.1,
  "citations_score": 99.9,
  "industry_income_score": 88.4,
  "international_outlook_score": 96.7
}
```

### Datos de Universidades Individuales

```json
{
  "url": "https://www.timeshighereducation.com/world-university-rankings/university-oxford",
  "name": "University of Oxford",
  "ranking_data": {
    "world_university_rankings_rank": "1",
    "reputation_ranking_rank": "3"
  },
  "key_stats": {
    "total_students": "24,515",
    "international_students": "42%",
    "faculty_count": "7,000",
    "student_faculty_ratio": "11:1"
  },
  "subjects": [
    {
      "name": "Arts & Humanities",
      "rank": "1",
      "score": "99.6"
    },
    {
      "name": "Clinical & Health",
      "rank": "2",
      "score": "97.8"
    }
  ],
  "additional_info": {
    "location": "Oxford, United Kingdom",
    "website": "https://www.ox.ac.uk",
    "description": "One of the world's oldest universities..."
  }
}
```

### Datos Combinados (Pipeline Completo)

```json
{
  "rank": 1,
  "name": "University of Oxford",
  "country": "United Kingdom",
  "university_url": "https://...",
  "overall_score": 98.5,
  "detailed_ranking_data": {
    "world_university_rankings_rank": "1"
  },
  "key_stats": {
    "total_students": "24,515"
  },
  "subjects": [...],
  "additional_info": {...}
}
```

## ğŸ”§ Casos de Uso EspecÃ­ficos

### 1. ConfiguraciÃ³n Inicial Completa

```bash
# ConfiguraciÃ³n automÃ¡tica completa
python setup.py

# Verificar instalaciÃ³n
python -m src --mode rankings-only --limit 5 --dry-run

# Primera ejecuciÃ³n completa de prueba
python -m src --mode full-pipeline --limit 10 --batch-size 5
```

```bash
# Scraping semanal automatizado
python -m src \
    --mode rankings-only \
    --config config/default_selenium.yml \
    --process-data

# Programar con cron (Linux/Mac)
# 0 2 * * 0 cd /path/to/scraper && python -m src --mode rankings-only
```

### 2. Monitoreo Regular de Rankings

```bash
# Scraping semanal automatizado
python -m src \
    --mode rankings-only \
    --config config/default_selenium.yml \
    --process-data

# Programar con cron (Linux/Mac)
# 0 2 * * 0 cd /path/to/scraper && python -m src --mode rankings-only
```

### 3. InvestigaciÃ³n AcadÃ©mica Profunda

```bash
# Scraping completo con mÃ¡ximo detalle
python -m src \
    --mode full-pipeline \
    --config config/full_pipeline.yml \
    --batch-size 20 \
    --export-data \
    --log-level DEBUG
```

### 3. IntegraciÃ³n con Sistemas Externos

```bash
# Exportar solo a PostgreSQL para dashboard
python -m src \
    --mode export-only \
    --processed-file data/processed/latest_data.pkl \
    --config config/full_pipeline.yml

# Solo PostgreSQL (sin archivos)
python -m src \
    --mode rankings-only \
    --config config/postgres_only.yml \
    --no-files  # (funciÃ³n futura)
```

### 4. AnÃ¡lisis de Universidades EspecÃ­ficas

```bash
# Crear archivo con URLs especÃ­ficas
echo "https://www.timeshighereducation.com/world-university-rankings/university-oxford" > specific_unis.txt
echo "https://www.timeshighereducation.com/world-university-rankings/harvard-university" >> specific_unis.txt

# Scraping solo de esas universidades
python scripts/scrape_universities.py \
    --urls-file specific_unis.txt \
    --config config/university_detail.yml
```

## ğŸ› Troubleshooting

### Problemas de ConfiguraciÃ³n Inicial

#### 1. Error en setup.py

```bash
# Problema: setup.py falla durante la ejecuciÃ³n
# SoluciÃ³n: Verificar permisos y ejecutar paso a paso

# Crear directorios manualmente
mkdir -p logs data/{raw,processed,universities,exports/{csv,excel,json},pipeline_results,backups} scripts

# Verificar estructura creada
ls -la data/
ls -la logs/
```

#### 2. Archivos de configuraciÃ³n faltantes

```bash
# Problema: Config files no encontrados despuÃ©s de setup.py
# SoluciÃ³n: Crear templates manualmente

# Crear config bÃ¡sico
cat > config/university_detail.yml << 'EOF'
general:
  output_dir: "data/universities"
  log_level: "INFO"
scraper:
  type: "selenium"
  headless: true
  university_delay: 5
EOF

# Verificar configuraciÃ³n
python -c "from src.core.config import load_config; print('Config OK')"
```

#### 3. Imports fallando despuÃ©s de setup

```bash
# Problema: ImportError para nuevos mÃ³dulos
# SoluciÃ³n: Verificar que los archivos estÃ¡n en las ubicaciones correctas

# Verificar estructura de archivos
find src/ -name "*.py" -type f | grep -E "(university_detail|__main__)"

# Verificar imports especÃ­ficos
python -c "
try:
    from src.scrapers.university_detail_scraper import UniversityDetailScraper
    print('âœ“ UniversityDetailScraper import OK')
except ImportError as e:
    print(f'âœ— Error: {e}')
"
```

### Errores Comunes

#### 4. Error de Selenium WebDriver

```bash
# Problema: WebDriver no encontrado
# SoluciÃ³n: Actualizar Chrome y webdriver-manager
pip install --upgrade webdriver-manager selenium
```

#### 5. Error de Parsing

```bash
# Problema: 'NoneType' object has no attribute 'text'
# SoluciÃ³n: Usar el parser mejorado (ya implementado)
# Verificar logs detallados:
python scripts/scrape_rankings.py --log-level DEBUG
```

#### 6. Timeout en Scraping

```bash
# Problema: PÃ¡ginas cargan lentamente
# SoluciÃ³n: Aumentar timeouts en configuraciÃ³n
# En config/university_detail.yml:
selenium:
  page_load_timeout: 120  # Aumentar de 60 a 120
  wait_timeout: 20       # Aumentar de 15 a 20
```

#### 7. LÃ­mite de Rate Limiting

```bash
# Problema: Servidor rechaza requests
# SoluciÃ³n: Aumentar delays
scraper:
  university_delay: 10   # Aumentar delay entre universidades
  request_delay: 5       # Aumentar delay general
```

### Logs y Debugging

#### UbicaciÃ³n de Logs

```bash
logs/
â”œâ”€â”€ main_orchestrator.log      # Log del orquestador principal
â”œâ”€â”€ rankings_scraper.log       # Log del scraping de rankings
â”œâ”€â”€ university_scraper.log     # Log del scraping de universidades
â””â”€â”€ selenium.log               # Log especÃ­fico de Selenium
```

#### Comandos de Debugging

```bash
# Ver logs en tiempo real
tail -f logs/main_orchestrator.log

# Buscar errores especÃ­ficos
grep -i error logs/*.log

# Ver estadÃ­sticas de scraping
grep -i "successfully" logs/*.log | wc -l
```

### VerificaciÃ³n de Datos

#### RevisiÃ³n RÃ¡pida de Resultados

```python
# Script para revisar datos procesados + PostgreSQL
import pandas as pd
import json
from pathlib import Path

# FunciÃ³n de verificaciÃ³n automÃ¡tica
def verify_installation():
    """Verificar que la instalaciÃ³n estÃ¡ completa."""

    # Verificar directorios
    required_dirs = [
        "data/raw", "data/processed", "data/universities",
        "data/exports/csv", "logs", "scripts"
    ]

    missing_dirs = []
    for dir_path in required_dirs:
        if not Path(dir_path).exists():
            missing_dirs.append(dir_path)

    if missing_dirs:
        print(f"âŒ Directorios faltantes: {missing_dirs}")
        print("ğŸ’¡ Ejecutar: python setup.py")
    else:
        print("âœ… Estructura de directorios completa")

    # Verificar archivos de configuraciÃ³n
    config_files = [
        "config/default_selenium.yml",
        "config/university_detail.yml",
        "config/full_pipeline.yml"
    ]

    missing_configs = []
    for config_file in config_files:
        if not Path(config_file).exists():
            missing_configs.append(config_file)

    if missing_configs:
        print(f"âŒ Configs faltantes: {missing_configs}")
    else:
        print("âœ… Archivos de configuraciÃ³n completos")

    # Verificar scripts
    script_files = [
        "scripts/scrape_rankings.py",
        "scripts/scrape_universities.py",
        "setup.py"
    ]

    missing_scripts = []
    for script_file in script_files:
        if not Path(script_file).exists():
            missing_scripts.append(script_file)

    if missing_scripts:
        print(f"âŒ Scripts faltantes: {missing_scripts}")
    else:
        print("âœ… Scripts completos")

    #  Verificar PostgreSQL
    try:
        import sys
        sys.path.insert(0, str(Path(__file__).parent / "src"))
        from src.storage.database_manager import PostgreSQLManager

        db_manager = PostgreSQLManager()
        if db_manager.test_connection():
            print("âœ… PostgreSQL conectado y funcionando")

            # Verificar datos en PostgreSQL
            stats = db_manager.get_scraping_stats()
            if stats:
                print(f"ğŸ“Š PostgreSQL - Rankings: {stats['total_rankings']}, Detalles: {stats['total_details']}")
            else:
                print("â„¹ï¸ PostgreSQL conectado pero sin datos aÃºn")

            db_manager.close()
        else:
            print("âš ï¸ PostgreSQL no disponible (opcional)")
    except Exception as e:
        print(f"â„¹ï¸ PostgreSQL no configurado: {e}")

# Ejecutar verificaciÃ³n
verify_installation()

# Revisar rankings (si existen)
rankings_files = list(Path("data/raw").glob("rankings_*.json"))
if rankings_files:
    latest_rankings = max(rankings_files, key=lambda x: x.stat().st_mtime)
    with open(latest_rankings) as f:
        rankings = json.load(f)
    print(f"âœ… Rankings: {len(rankings)} universidades en {latest_rankings.name}")
else:
    print("â„¹ï¸ No hay archivos de rankings aÃºn")

# Revisar universidades (si existen)
uni_files = list(Path("data/universities").glob("universities_detail_*.json"))
if uni_files:
    latest_unis = max(uni_files, key=lambda x: x.stat().st_mtime)
    with open(latest_unis) as f:
        universities = json.load(f)
    print(f"âœ… Universidades: {len(universities)} con detalles en {latest_unis.name}")
else:
    print("â„¹ï¸ No hay archivos de universidades aÃºn")

# Revisar datos procesados (si existen)
processed_files = list(Path("data/processed").glob("*.pkl"))
if processed_files:
    latest_processed = max(processed_files, key=lambda x: x.stat().st_mtime)
    df = pd.read_pickle(latest_processed)
    print(f"âœ… Datos procesados: {df.shape} en {latest_processed.name}")
    print(f"   Columnas: {list(df.columns)}")
else:
    print("â„¹ï¸ No hay datos procesados aÃºn")
```

## ğŸ“ˆ Rendimiento y OptimizaciÃ³n

### ConfiguraciÃ³n de Rendimiento

```yaml
# En full_pipeline.yml
pipeline:
  university_batch_size: 100 # Aumentar para mÃ¡s velocidad
  max_concurrent_requests: 5 # ParalelizaciÃ³n (futuro)

scraper:
  headless: true # MÃ¡s rÃ¡pido sin UI
  page_load_timeout: 30 # Reducir si la conexiÃ³n es buena
```

### MÃ©tricas de Rendimiento TÃ­picas (Actualizadas)
- **Rankings**: ~2000 universidades en 10-15 minutos
- **Universidades**: ~50 universidades por lote en 5-8 minutos  
- **Pipeline Completo**: 500 universidades en 45-60 minutos
- **Procesamiento**: <1 minuto para datasets tÃ­picos
- **ğŸ”¥ Auto-inserciÃ³n PostgreSQL**: +5-10 segundos por lote (despreciable)
- **ğŸ”¥ VerificaciÃ³n PostgreSQL**: <2 segundos

## ğŸ”„ Mantenimiento y Actualizaciones

### Archivos Creados AutomÃ¡ticamente por setup.py

El script `setup.py` crea automÃ¡ticamente:

#### **Estructura de Directorios**

```
ğŸ“ logs/                    # Para archivos de log
ğŸ“ data/
  â”œâ”€â”€ ğŸ“ raw/              # HTML y JSON originales
  â”œâ”€â”€ ğŸ“ processed/        # Datos procesados
  â”œâ”€â”€ ğŸ“ universities/     # Detalles de universidades
  â”œâ”€â”€ ğŸ“ exports/          # Datos exportados
  â”‚   â”œâ”€â”€ ğŸ“ csv/         # Exportaciones CSV
  â”‚   â”œâ”€â”€ ğŸ“ excel/       # Exportaciones Excel
  â”‚   â””â”€â”€ ğŸ“ json/        # Exportaciones JSON
  â”œâ”€â”€ ğŸ“ pipeline_results/ # Resultados de pipeline
  â””â”€â”€ ğŸ“ backups/         # Backups automÃ¡ticos
ğŸ“ scripts/                # Scripts especializados
```

#### **Archivos de ConfiguraciÃ³n**

```
ğŸ“„ .env.template           # Template de variables de entorno
ğŸ“„ .gitignore             # ConfiguraciÃ³n de Git
ğŸ“„ data/.gitkeep          # Preservar estructura en Git
ğŸ“„ logs/.gitkeep          # Preservar estructura en Git
```

#### **Variables de Entorno Template**

El archivo `.env.template` creado incluye:

```bash
# PostgreSQL settings
POSTGRES_PASSWORD=your_secure_password_here
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=university_rankings
POSTGRES_USER=scraper_user

# Optional: API keys for future integrations
# THE_API_KEY=your_api_key_here

# Optional: Notification settings
# WEBHOOK_URL=https://your-webhook-url.com
# EMAIL_ALERTS_TO=admin@yourcompany.com
```

### Actualizaciones Regulares

```bash
# Actualizar dependencias
poetry update

# Re-ejecutar setup para verificar estructura
python setup.py

# Verificar compatibilidad con el sitio web
python scripts/scrape_rankings.py --dry-run --log-level DEBUG
```

#### **ActualizaciÃ³n Mayor** (cambios estructurales)

```bash
# 1. Backup completo
tar -czf full_backup_$(date +%Y%m%d).tar.gz .

# 2. Re-ejecutar setup.py para nuevas estructuras
python setup.py

# 3. Migrar configuraciones si es necesario
# 4. Prueba extensiva antes de producciÃ³n
```

## ğŸš€ GuÃ­a de Inicio RÃ¡pido

### DespuÃ©s de Ejecutar setup.py

#### 1. **Verificar InstalaciÃ³n**

```bash
# Verificar que setup.py completÃ³ correctamente
python -c "
from pathlib import Path
required = ['logs', 'data/raw', 'data/universities', 'scripts']
missing = [d for d in required if not Path(d).exists()]
if missing:
    print(f'âŒ Faltantes: {missing}')
    print('ğŸ’¡ Re-ejecutar: python setup.py')
else:
    print('âœ… Estructura completa')
"
```

#### 2. **Primera EjecuciÃ³n (Prueba)**

```bash
# Prueba bÃ¡sica - solo verificar configuraciÃ³n
python -m src --mode rankings-only --limit 3 --dry-run --log-level DEBUG

# Primera ejecuciÃ³n real con datos mÃ­nimos
python -m src --mode rankings-only --limit 5 --log-level INFO
```

#### 3. **Verificar Resultados**

```bash
# Verificar archivos generados
ls -la data/raw/rankings_*.json
ls -la data/universities/*.json | tail -3
ls -la logs/*.log

# Verificar logs por errores
grep -c "ERROR\|CRITICAL" logs/*.log

# Verificar estadÃ­sticas de scraping
grep -i "successfully" logs/*.log | wc -l
```

#### 4. **Pipeline Completo de Prueba**

```bash
# Ejecutar pipeline completo con datos limitados
python -m src --mode full-pipeline --limit 10 --batch-size 5 --log-level INFO
```

### Comandos de VerificaciÃ³n Post-Setup

#### **VerificaciÃ³n de Integridad**

```bash
# Script de verificaciÃ³n completa
python -c "
import sys
from pathlib import Path

def check_files():
    checks = {
        'Directorios': [
            'logs', 'data/raw', 'data/universities', 'scripts',
            'data/exports/csv', 'data/exports/json'
        ],
        'Configs': [
            'config/default_selenium.yml',
            'config/university_detail.yml',
            'config/full_pipeline.yml'
        ],
        'Scripts': [
            'scripts/scrape_rankings.py',
            'scripts/scrape_universities.py',
            'setup.py'
        ],
        'Source': [
            'src/__main__.py',
            'src/parsers/rankings_parser.py',
            'src/scrapers/university_detail_scraper.py'
        ]
    }

    all_good = True
    for category, files in checks.items():
        print(f'\\n{category}:')
        for file_path in files:
            if Path(file_path).exists():
                print(f'  âœ… {file_path}')
            else:
                print(f'  âŒ {file_path}')
                all_good = False

    if all_good:
        print('\\nğŸ‰ Â¡InstalaciÃ³n completa y verificada!')
        print('\\nğŸš€ Listo para ejecutar:')
        print('   python -m src --mode full-pipeline --limit 10')
    else:
        print('\\nâš ï¸  Faltan archivos. Revisar instalaciÃ³n.')

check_files()
"
```

#### **Test de Importaciones**

```bash
# Verificar que todas las importaciones funcionan
python -c "
try:
    from src.core.pipeline import ScrapingPipeline
    from src.core.university_pipeline import UniversityDetailPipeline
    from src.scrapers.university_detail_scraper import UniversityDetailScraper
    from src.parsers.university_detail_parser import UniversityDetailParser
    print('âœ… Todas las importaciones exitosas')
    print('ğŸ¯ Sistema listo para usar')
except ImportError as e:
    print(f'âŒ Error de importaciÃ³n: {e}')
    print('ğŸ’¡ Verificar que todos los archivos estÃ¡n copiados correctamente')
"
```

#### **Test de ConfiguraciÃ³n**

```bash
# Verificar configuraciones
python -c "
from src.core.config import load_config
from pathlib import Path

configs = [
    'config/default_selenium.yml',
    'config/university_detail.yml',
    'config/full_pipeline.yml'
]

for config_file in configs:
    try:
        if Path(config_file).exists():
            config = load_config(Path(config_file))
            print(f'âœ… {config_file} - OK')
        else:
            print(f'âŒ {config_file} - No existe')
    except Exception as e:
        print(f'âŒ {config_file} - Error: {e}')
"
```

### Desarrollo Local

```bash
# Fork del repositorio
git clone <your-fork>
cd world_university_scraper

# Crear rama de feature
git checkout -b feature/nueva-funcionalidad

# Ejecutar tests (cuando estÃ©n disponibles)
poetry run pytest

# Commit y push
git add .
git commit -m "feat: nueva funcionalidad"
git push origin feature/nueva-funcionalidad
```

### Reportar Issues

Incluir en el reporte:

1. Comando ejecutado
2. Logs relevantes
3. ConfiguraciÃ³n utilizada
4. VersiÃ³n de Python y dependencias

## ğŸ“š Recursos Adicionales

### Scripts y Herramientas Incluidas

#### **setup.py - ConfiguraciÃ³n Inicial**

- âœ… Crea estructura completa de directorios
- âœ… Genera archivos de configuraciÃ³n template
- âœ… Configura .gitignore y .env.template
- âœ… Valida instalaciÃ³n y muestra siguientes pasos

### Mejores PrÃ¡cticas Establecidas

#### **Antes de Cada EjecuciÃ³n**

```bash
# 1. Verificar espacio libre
df -h .

# 2. Verificar estructura
python setup.py >/dev/null 2>&1 && echo "âœ… Setup OK" || echo "âŒ Re-ejecutar setup.py"

# 3. Limpiar logs antiguos si es necesario
find logs/ -name "*.log" -mtime +7 -delete
```

#### **DespuÃ©s de Cada EjecuciÃ³n**

```bash
# 1. Verificar resultados
ls -la data/raw/*.json | tail -3
ls -la data/universities/*.json | tail -3

# 2. Revisar errores
grep -c "ERROR\|CRITICAL" logs/*.log

# 3. Backup si datos importantes
[ -f "data/processed/combined_data_latest.pkl" ] && cp data/processed/combined_data_latest.pkl data/backups/
```

**Â¡El sistema estÃ¡ completamente documentado y listo para uso en producciÃ³n!**

### ğŸ‰ Â¡Comenzar Ahora!

```bash
# ğŸ”¥ COMANDO ÃšNICO PARA EMPEZAR (CON POSTGRESQL)
cd data && docker-compose up -d postgres && cd .. && \
python setup.py && \
python -m src --mode full-pipeline --limit 10 --log-level INFO && \
poetry run python verify_database.py
```

```bash
# ğŸ”¥ ALTERNATIVA SIN POSTGRESQL
python setup.py && python -m src --mode full-pipeline --limit 10 --no-postgres
```
